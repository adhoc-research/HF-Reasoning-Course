{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d7d35e",
   "metadata": {},
   "source": [
    "### Finetuning SmolLM2 with GRPO and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6171bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rushat/anaconda3/envs/transformers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47151063",
   "metadata": {},
   "source": [
    "For this experiment, we will be using the `smoltldr` dataset.\n",
    "\n",
    "Smoltdlr contains a list of 2000 short stories from Reddit in the 'prompt' column along with their respective summaries in the 'completion' column. The goal of this experiment is to see if we can successfully finetune our large language model using GRPO to summarize reddit posts in a similar fashion to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c707e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 2000/2000 [00:00<00:00, 52769.16 examples/s]\n",
      "Generating validation split: 100%|██████████| 200/200 [00:00<00:00, 60519.50 examples/s]\n",
      "Generating test split: 100%|██████████| 200/200 [00:00<00:00, 58514.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"mlabonne/smoltldr\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60d642",
   "metadata": {},
   "source": [
    "The \"large\" language model we'll be using in SmolLM2-135M. As the name suggests, this is a small model with only 135M parameters (as compared to the billions in today's leading-edge LLMs). Its size makes it feasible for us to run/finetune it on limited hardware for educational purposes, but it will not be useful for anything practical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d182309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b5675",
   "metadata": {},
   "source": [
    "The next step is to load the LoRA configuration. Using LoRA, we can reduce the number of trainable parameters we need to fine-tune the model - effectively reducing its memory footpint.\n",
    "\n",
    "What is LoRA?\n",
    "\n",
    "LoRA stands for \"Low-Rank Adaptation.\" In brief, it is a parameter-efficient finetuning technique that modifies only a small portion of the model's weights. Instead of updating all the model’s parameters, LoRA decomposes the weight updates into low-rank matrices which \"nudge\" the model behavior in the desired direction through low-rank decomposition, and finetunes those instead.\n",
    "\n",
    "Step-by-step:\n",
    "1. Freezes the original model weights W so they don't get updated while finetuning/training\n",
    "2. Chooses target layers to inject LoRA into\n",
    "3. For each target layer, adds two low-rank trainable matrics A and B such that a change in W can be mapped to them (delta(W) = A * B)\n",
    "4. Only LoRA parameters A and B are updated and saved during finetuning\n",
    "5. During inference forward passes, it will load the frozen base model alongside the LoRA adapter weights, and compose the original weights + LoRA to compose answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35a6572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,884,480 || all params: 139,399,488 || trainable%: 3.5039\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f428158",
   "metadata": {},
   "source": [
    "Now we need to define the reward function. GRPO is flexible and can use any reward function to improve the model. In this case, we'll be using a simple reward function that encourages the model to generate text that is 50 tokens long - optimal for our summarization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89644beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function\n",
    "ideal_length = 50\n",
    "\n",
    "\n",
    "def reward_len(completions, **kwargs):\n",
    "    return [-abs(ideal_length - len(completion)) for completion in completions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bdf8f",
   "metadata": {},
   "source": [
    "In this step we use the `GRPOConfig` class to define the training arguments. Important ones are as follows:\n",
    "1. Learning Rate: Controls how fast the model learns. As with any neural network, too high = unstable training, too low = slow convergence\n",
    "2. Number of Training Epochs: Similar to ANNs, more epochs = more learning with risk of overfitting, less epochs = faster learning but risk of underfitting\n",
    "3. bf16: bfloat16 precision enables faster and more memory-efficient training on hardware that supports it (like TPUs or newer GPUs)\n",
    "4. Optimizer: `adamw_8bit` is efficient for large models using 8-bit optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1a01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"GRPO\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_prompt_length=512,\n",
    "    max_completion_length=96,\n",
    "    num_generations=8,\n",
    "    optim=\"adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e17120",
   "metadata": {},
   "source": [
    "Finally, we can initialize the trainer with model, dataset and training arguments to start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0009acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[reward_len],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
